## CS47100 Study Guide: Probability, Bayesian Networks, MDPs, and RL

***

### I. Probability and Uncertainty Fundamentals

| Concept | Description | Formula / Key Principle |
| :--- | :--- | :--- |
| **Uncertainty in AI** | Traditional logic systems are brittle in noisy, real-world environments. Probabilistic reasoning provides a framework for managing beliefs given evidence. | N/A |
| **Random Variables (RVs)** | A function that maps possible outcomes ($\Omega$) to a set of values. | E.g., $D \in \{2, \ldots, 12\}$ (sum of two dice). |
| **Probability Distribution** | An assignment of weights (probabilities) to outcomes. | Must satisfy: $P(X=x) \ge 0$ and $\sum_{x} P(X=x) = 1$. |
| **Joint Distribution** | Specifies a probability for every possible combination (assignment) of outcomes for a set of random variables. | The size grows exponentially $O(d^n)$, making explicit storage impractical. |
| **Marginalization** | The process of removing or eliminating variables from a joint distribution by summing out their values. | $P(X_1=x_1) = \sum_{x_2} P(X_1=x_1, X_2=x_2)$. |
| **Conditional Probability** | The likelihood of event $a$ given event $b$ has occurred. | $P(a|b) = \frac{P(a, b)}{P(b)}$. |
| **Chain Rule** | A fundamental rule allowing any joint distribution to be factored into a product of conditional distributions. | $P(x_1, x_2, \ldots x_n) = \prod_{i=1}^{n} P(x_i | x_1 \ldots x_{i-1})$. |
| **Independence** | Two variables $X$ and $Y$ are independent if observing one gives no information about the other. (Unconditional independence is "very rare"). | $P(x, y) = P(x)P(y)$ or $P(x|y) = P(x)$. |
| **Conditional Independence** | The core concept of many probabilistic models. $X$ and $Y$ are independent if $Z$ is known. | $P(x, y|z) = P(x|z)P(y|z)$. |
| **Bayes' Rule** | Relates conditional probabilities, often used to find diagnostic probability $P(\text{cause}|\text{effect})$ from causal probability $P(\text{effect}|\text{cause})$. | $P(\text{cause}|\text{effect}) = \frac{P(\text{effect}|\text{cause})P(\text{cause})}{P(\text{effect})}$. |

***

### II. Bayesian Networks (BNs)

| Concept | Description | Formula / Key Principle |
| :--- | :--- | :--- |
| **BN Structure** | A directed acyclic graph (DAG) composed of: 1) **Nodes** (Random Variables) and 2) **Edges** (representing direct influence/conditional dependency). | BN = **Topology (Graph) + Local Conditional Probabilities**. |
| **BN Semantics** | The network implicitly encodes the full joint distribution over all variables. | $P(x_1, x_2, \ldots x_n) = \prod_{i=1}^{n} P(x_i|\text{parents}(X_i))$. |
| **Independence Assumption** | The BN structure assumes that each node $X_i$ is **conditionally independent of its non-descendants given its parents**. | The general Chain Rule simplifies to the BN Joint Distribution Formula under this assumption. |
| **D-Separation** | An algorithm used to query the graph about conditional independence relationships between any two variables given observed evidence (a set of nodes $Z$). | **If all paths** between X and Y are **inactive** given Z, then $X \perp Y|Z$ is guaranteed. |
| **Causal Chain** | $X \rightarrow Y \rightarrow Z$. If $Y$ is **observed**, the path is **inactive**; $X$ and $Z$ are independent given $Y$. |
| **Common Cause** | $X \leftarrow Y \rightarrow Z$. If $Y$ is **observed**, the path is **inactive**; $X$ and $Z$ are independent given $Y$. |
| **Common Effect** **(V-Structure)**| $X \rightarrow Z \leftarrow Y$. $X$ and $Y$ are **unconditionally independent**. If $Z$ (or a descendant of $Z$) is **observed**, the path becomes **active**, inducing dependence between $X$ and $Y$. |

***

### III. Bayesian Network Inference

| Method | Type | Principle / How it Works | Complexity |
| :--- | :--- | :--- | :--- |
| **Inference by Enumeration** | Exact | Computes desired conditional probability $P(Q|e)$ by accessing the full joint distribution, selecting entries matching evidence, summing out hidden variables ($H$), and normalizing. | Time: $O(d^n)$, often intractable due to redundant computations. |
| **Variable Elimination (VE)**| Exact | Optimizes enumeration by using intermediate results called **factors** and intelligently interleaving joining (multiplication/pointwise product) and marginalizing (summation) early to avoid repetition. | Still NP-hard, but significantly faster than raw enumeration. |
| **Simple Sampling** | Approximate | Generates samples from the network's CPDs sequentially to build an empirical distribution approximating the joint distribution. | Efficient for networks with no evidence. |
| **Rejection Sampling** | Approximate | Samples the network but **discards** any samples that do not match the observed evidence. | Hopelessly expensive if the probability of the evidence $P(e)$ is very small. |
| **Likelihood Weighting** | Approximate | Generates samples consistent with the evidence but assigns a **weight** to each sample based on the likelihood of the observed evidence occurring. | More efficient than rejection sampling as no samples are discarded. |

***

### IV. Markov Decision Processes (MDPs)

| Concept | Description | Formula / Key Principle |
| :--- | :--- | :--- |
| **MDP Formulation** | A formal model for agents operating in a stochastic environment where actions have uncertain outcomes. | Defined by: **States** $S$, **Actions** $A$, **Transition/Model** $P(s'|s, a)$, and **Reward** $R(s, a, s')$. |
| **Markov Property** | The future state depends only on the current state and action, not on the entire history of preceding states or actions. | $P(S_{t+1}=s'|S_t=s_t, A_t=a_t, \ldots) = P(S_{t+1}=s'|S_t=s_t, A_t=a_t)$. |
| **Goal of MDP** | To find the **optimal policy** $\pi^*(s)$, a function that maps every state to the optimal action to take in that state. | $\pi^*: S \to A$. |
| **Utility / Discounting** | Utility is measured by the expected sum of discounted rewards. Rewards now are preferred over rewards later. | **Discounted Utility:** $U([r_0, r_1, r_2, \ldots]) = \sum_{t=0}^{\infty} \gamma^t r_t$, where $0 < \gamma < 1$. |
| **Optimal Value ($V^*$)** | The expected utility (sum of discounted rewards) achieved starting in state $s$ and acting optimally thereafter. | N/A |
| **Optimal Q-Value ($Q^*$)**| The expected utility achieved starting in state $s$, taking action $a$, and acting optimally thereafter. | N/A |

***

### V. MDP Planning Algorithms (Offline Solution)

| Algorithm / Concept | Goal | Update Rule / Key Step |
| :--- | :--- | :--- |
| **Bellman Equation** | Recursive definition linking the optimal value of a state to the expected utility of the best possible action. | $V^*(s) = \max_a \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V^*(s')]$. |
| **Value Iteration (VI)**| Compute optimal values $V^*$ by repeatedly applying the Bellman update starting from $V_0(s)=0$ until convergence. | $V_{k+1}(s) \leftarrow \max_a \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V_k(s')]$. |
| **Policy Extraction** | Determine the optimal policy $\pi^*$ once the optimal values $V^*$ are found. | $\pi^*(s) = \operatorname{argmax}_a \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V^*(s')]$. |
| **Policy Evaluation** | Compute the values $V^\pi(s)$ achieved by following a **fixed, non-optimal policy** $\pi$. | $V^{\pi}_{k+1}(s) \leftarrow \sum_{s'} P(s'|s, \pi(s))[R(s, \pi(s), s') + \gamma V^{\pi}_k(s')]$. |
| **Policy Iteration (PI)**| Computes $\pi^*$ by alternating two phases: 1) Policy Evaluation (finding $V^\pi$) and 2) Policy Improvement (extracting a better $\pi'$ from $V^\pi$). | Policies usually converge much faster than values. |

***

### VI. Reinforcement Learning (RL)

| RL Type / Concept | Problem / Context | Learning Mechanism / Key Idea |
| :--- | :--- | :--- |
| **RL Distinction** | The agent assumes an MDP structure, but the transition function ($\mathbf{T}$) and/or reward function ($\mathbf{R}$) are **unknown**. | Agent must **explore** the environment by taking actions to learn about $T$ and $R$. |
| **Passive RL** | **Fixed Policy $\pi$**: Goal is only policy evaluation (learn $V^\pi(s)$). | **Temporal Difference (TD) Learning**: Uses observed transition samples $(s, a, s', r)$ to update $V(s)$ via a running average. |
| **Active RL** | **Agent chooses actions**: Goal is finding $\pi^*(s)$ while facing the **exploration vs. exploitation** tradeoff. | **Q-Learning** (Model-Free). |
| **Q-Learning Update** | Sample-based Q-value iteration that learns the optimal Q-values directly from experience. | $Q(s, a) \leftarrow (1-\alpha)Q(s, a) + \alpha[\mathbf{R}(s, a, s') + \gamma \max_{a'} Q(s', a')]$. |
| **Off-Policy Learning** | Q-Learning converges to the optimal $Q^*(s, a)$ even if the agent is acting suboptimally (i.e., exploring randomly). | $\epsilon$-Greedy policies force exploration by choosing random actions with probability $\epsilon$. |
| **Approximate Q-Learning**| Required when the state space is too large to store all Q-values. | Uses **linear value functions** defined by a weighted combination of state features, enabling generalization across similar states. |

***

### VII. Supervised Learning

| Concept | Description | Learning Process |
| :--- | :--- | :--- |
| **Supervised Learning**| Learn a mapping/model $f_\theta$ from input features ($\mathbf{x}$) to target outputs ($\mathbf{y}$) given input-output training pairs. | **Classification** (categorical $y$) or **Regression** (real-valued $y$). |
| **Maximum Likelihood (MLE)** | A principle for parameter learning ($\theta$) that chooses parameters maximizing the likelihood of observing the training data. | For simple categorical models, MLE usually reduces to **counting** observed frequencies. |
| **Naïve Bayes** | A probabilistic classification model assuming that all features ($X_k$) are **conditionally independent given the class label $Y$**. | Prediction involves computing the posterior $P(Y|\mathbf{x}) \propto P(Y) \prod_k P(X_k|Y)$ using parameters learned via counting. |
| **Overfitting** | Occurs when a model achieves high accuracy on training data but fails to **generalize** to new, unseen test data. | E.g., memorizing the training set leads to $100\%$ training accuracy but is useless. |
| **Laplace Smoothing**| A technique used (especially in Naïve Bayes) to combat overfitting by preventing zero probabilities. | It estimates probabilities by pretending every outcome was observed $k$ extra times. |
| **Model Evaluation Cycle**| Necessary steps to ensure generalization and avoid overfitting to training data. | 1) Train parameters ($\theta$) on **Training Data**. 2) Tune hyperparameters ($k$) on separate **Validation Data**. 3) Test final performance on separate **Test Data**. |